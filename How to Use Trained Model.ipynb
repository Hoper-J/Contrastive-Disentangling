{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83ab5789-022c-4f96-97d1-ffda53e0d196",
   "metadata": {},
   "source": [
    "# How to Use Trained Model: A Guide\n",
    "\n",
    "This notebook provides a step-by-step guide on how to load and use a pre-trained model for downstream tasks. Whether you're looking to fine-tune the model for a specific task, perform inference, or explore the features learned by the model, this notebook will help you get started.\n",
    "\n",
    "## Objectives:\n",
    "- **Load a Pre-Trained Model:** Learn how to load a model that has been trained using the provided scripts.\n",
    "- **Prepare Data for Evaluation and Visualization:** Set up your dataset specifically for assessing the model’s performance and visualizing its learned features.\n",
    "- **Evaluate Performance:** Assess the model’s performance on your dataset using standard metrics.\n",
    "- **Visualize Model Features:** Visualize the features extracted by the model.\n",
    "- **Extract Features Using the Model's Backbone:** Learn how to extract and utilize the backbone of the pre-trained model for feature extraction.\n",
    "- **Implement a Toy Example of Supervised Classification:** See a practical demonstration of how to use the backbone of the pre-trained model for a simple supervised classification task.\n",
    "- **Customize for Your Needs:** Extend this notebook to suit your specific requirements.\n",
    "\n",
    "## Prerequisites:\n",
    "- A trained model saved using the training scripts provided in this project.\n",
    "\n",
    "## Contents:\n",
    "1. [Loading the Pre-Trained Model](#loading-the-pre-trained-model)\n",
    "2. [Preparing Data for Evaluation and Visualization](#preparing-data-for-evaluation-and-visualization)\n",
    "3. [Evaluating Performance](#evaluating-performance)\n",
    "4. [Visualizing Model Features](#visualizing-model-features)\n",
    "5. [Extracting the Backbone for Downstream Tasks](#extracting-the-backbone-for-downstream-tasks)\n",
    "6. [Toy Example: Supervised Classification with the Backbone](#toy-example-supervised-classification-with-the-backbone)\n",
    "7. [Customizing for Your Needs](#customizing-for-your-needs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6766b-b6a3-43ed-a0e4-82121e0ed5fc",
   "metadata": {},
   "source": [
    "<a id=\"loading-the-pre-trained-model\"></a>\n",
    "## 1. Loading the Pre-Trained Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac5fcaad-0295-401b-ae91-4d0ca012504d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from modules.network import Network\n",
    "\n",
    "# Define the model parameters (ensure these match the parameters used during training)\n",
    "model_path = 'models/cifar10/model_epoch_1000.pth'\n",
    "backbone = 'ResNet34'\n",
    "feature_num = 128\n",
    "hidden_dim = 128\n",
    "\n",
    "# Define the device to be used for computation (GPU if available, otherwise CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# For Apple Silicon users: Check if 'mps' backend is available and use it if possible\n",
    "if device == torch.device('cpu') and hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "\n",
    "# Initialize the model architecture\n",
    "model = Network(backbone=backbone, feature_num=feature_num, hidden_dim=hidden_dim)\n",
    "\n",
    "# Load the trained model weights\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eca0e2-6f58-4b69-8367-24606819c65e",
   "metadata": {},
   "source": [
    "<a id=\"preparing-data-for-evaluation-and-visualization\"></a> \n",
    "## 2. Preparing Data for Evaluation and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "498aa23b-795d-4453-bd2b-7db5a8c1e347",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Data loaders prepared successfully!\n"
     ]
    }
   ],
   "source": [
    "from dataset import get_data_loader\n",
    "\n",
    "# Define the dataset and other parameters (ensure these match what was used during training)\n",
    "config = {\n",
    "    'dataset': 'cifar10',\n",
    "    'batch_size': 128,\n",
    "    's': 1.0,            # Color jitter strength (default)\n",
    "    'blur': False,       # Whether to apply Gaussian blur (default)\n",
    "    'customize_datasets': False  # Whether to use custom datasets\n",
    "}\n",
    "\n",
    "# Load the train, test data loaders\n",
    "train_loader, test_loader, _ = get_data_loader(config)\n",
    "\n",
    "print(\"Data loaders prepared successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5126c1-fd4c-4e01-9339-c7a6a7e5aeb7",
   "metadata": {},
   "source": [
    "<a id=\"measuring-metrics\"></a>\n",
    "## 3. Measuring Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19fb50db-eb10-45b0-96dd-5446266259e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backbone NMI: 0.7249, ARI: 0.6199, ACC: 0.7992\n",
      "Feature NMI: 0.7109, ARI: 0.6243, ACC: 0.7876\n"
     ]
    }
   ],
   "source": [
    "from utils.metrics import evaluate\n",
    "\n",
    "# Perform evaluation on the test data and measure the metrics\n",
    "nmi_backbone, ari_backbone, acc_backbone, nmi_feature, ari_feature, acc_feature = evaluate(model, test_loader, device=device)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Backbone NMI: {nmi_backbone:.4f}, ARI: {ari_backbone:.4f}, ACC: {acc_backbone:.4f}\")\n",
    "print(f\"Feature NMI: {nmi_feature:.4f}, ARI: {ari_feature:.4f}, ACC: {acc_feature:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0cd4c7-41e5-45e6-8f44-c2ab7af17ae3",
   "metadata": {},
   "source": [
    "<a id=\"visualizing-model-features\"></a>\n",
    "## 4. Visualizing Model Features\n",
    "\n",
    "We'll display the t-SNE plots for both the backbone and feature embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "983c3c7b-5d71-4aff-a91a-da8421cae613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-SNE visualizations generated!\n"
     ]
    }
   ],
   "source": [
    "from utils.visualization import visualize_embeddings\n",
    "\n",
    "# Visualize embeddings using t-SNE\n",
    "# The first image corresponds to the backbone, and the second image corresponds to the feature predictor.\n",
    "visualize_embeddings(model, test_loader, device=device, epoch=1000, name='cifar10')\n",
    "\n",
    "print(\"t-SNE visualizations generated!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcba84f-d33c-4a55-a872-5163f071bd00",
   "metadata": {},
   "source": [
    "<a id=\"extracting-the-backbone-for-downstream-tasks\"></a> \n",
    "## 5. Extracting the Backbone for Downstream Tasks\n",
    "\n",
    "Let's explore how to access and use the backbone from the pre-trained model. The backbone, which is typically a feature extractor like ResNet, can be leveraged for various downstream tasks such as classification, detection, or segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46922fbc-f869-47b7-8ef8-b2cc1cab3702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features shape: torch.Size([128, 512])\n"
     ]
    }
   ],
   "source": [
    "# Access the backbone from the model\n",
    "backbone = model.resnet\n",
    "\n",
    "# Example: Extract features using the backbone\n",
    "with torch.no_grad():\n",
    "    for (x, _, _) in train_loader:\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # Pass the input through the backbone to extract features\n",
    "        features = backbone(x)\n",
    "\n",
    "        # Display the shape of the extracted features\n",
    "        print(f\"Extracted features shape: {features.shape}\")\n",
    "        \n",
    "        # Break after the first batch to avoid unnecessary computations\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22846f-cd06-4ca5-923f-b9dd8601eb5d",
   "metadata": {},
   "source": [
    "<a id=\"toy-example-supervised-classification-with-the-backbone\"></a>\n",
    "## 6. Toy Example: Supervised Classification with the Backbone\n",
    "\n",
    "In this section, we will utilize the backbone of the pre-trained model to perform supervised classification on the current dataset. This is a toy example designed to demonstrate the basic steps involved: extracting features using the backbone, adding a simple classification head, training the model on the dataset, and evaluating its accuracy.\n",
    "\n",
    "This example uses a small number of epochs and a simple dataset to illustrate the process, making it easy to follow along and adapt for more complex scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf5e68-4675-468e-a053-426acee1475a",
   "metadata": {},
   "source": [
    "### 6.1 Preparing Data\n",
    "Define the transformations and load the CIFAR-10 dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc3368a5-d118-41fa-ad9b-a9a8a1edb5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the transformations for training and testing\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(size=224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load datasets with the defined transforms\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
    "\n",
    "# Define the data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8a5dd8-5277-4806-b901-9d240a0e553d",
   "metadata": {},
   "source": [
    "### 6.2 Defining the Model\n",
    "Next, we define a toy classifier model that uses the backbone for feature extraction, followed by a fully connected layer for classification.\n",
    "\n",
    "The backbone of a pre-trained model is typically used to extract meaningful features from the data. By removing the final layers, we can repurpose these features for a new task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92190dd-b8a9-49d9-b51a-a3a159df98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Define a simple classifier using the backbone\n",
    "class ToyClassifier(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(ToyClassifier, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # Freeze the backbone\n",
    "            features = self.backbone(x)\n",
    "        out = self.fc(features)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6122a6-a232-4e10-ad4c-a5e0d819a5df",
   "metadata": {},
   "source": [
    "### 6.3 Setting Up Training\n",
    "Set up the loss function, optimizer, and the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd4f18db-22fa-49b7-8036-0bb8f601f83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [00:59<00:00, 26.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5063, Accuracy: 84.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [01:00<00:00, 25.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 0.4125, Accuracy: 85.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [00:59<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/5], Loss: 0.4016, Accuracy: 86.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [00:59<00:00, 26.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/5], Loss: 0.3957, Accuracy: 86.58%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/5: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1563/1563 [00:59<00:00, 26.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/5], Loss: 0.3940, Accuracy: 86.64%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Assuming you have a dataset and dataloaders ready\n",
    "num_classes = 10  # Set this to the number of classes in your dataset\n",
    "model = ToyClassifier(backbone, num_classes).to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop for 5 epochs\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Iterate over the training data\n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, y)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update running loss and accuracy\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        predictions = outputs.argmax(1)\n",
    "        total += y.size(0)\n",
    "        correct += predictions.eq(y).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / total\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {epoch_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e9a441-6034-4340-ab45-83fd9090532f",
   "metadata": {},
   "source": [
    "### 6.4 Evaluating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40591211-0000-42a1-948a-9942960b5f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████| 313/313 [00:11<00:00, 27.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 91.32%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Initialize counters for correct predictions and total samples\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient calculation for evaluation\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        # Move input data and labels to the appropriate device\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        # Get model predictions\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # Get the predicted class by taking the index with the highest score\n",
    "        predictions = outputs.argmax(1)\n",
    "        \n",
    "        # Update total count and correct predictions\n",
    "        total += y.size(0)\n",
    "        correct += predictions.eq(y).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "test_accuracy = 100. * correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b168a8b4-66b6-4145-9a9d-05532f2f4d07",
   "metadata": {},
   "source": [
    "<a id=\"customizing-for-your-needs\"></a>\n",
    "## 7. Customizing for Your Needs\n",
    "\n",
    "This notebook just provides a starting point. You can customize it by:\n",
    "- Implementing additional downstream tasks, such as fine-tuning the model.\n",
    "- Integrating the model into a larger pipeline.\n",
    "\n",
    "We hope this notebook helps you get started. Feel free to modify it to suit your specific needs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ba1cf7-1287-4722-92dd-7c03c424e748",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
